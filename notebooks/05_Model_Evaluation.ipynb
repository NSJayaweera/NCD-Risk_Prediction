{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Model Evaluation\n", "\n", "Evaluate model performance using various metrics and visualizations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 1: Load Trained Models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import joblib\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n", "                             roc_auc_score, confusion_matrix, roc_curve, classification_report)\n", "\n", "# Load training data\n", "training_data = joblib.load('models/training_data.pkl')\n", "\n", "male_model = training_data['male_model']\n", "female_model = training_data['female_model']\n", "X_test_male = training_data['X_test_male']\n", "X_test_female = training_data['X_test_female']\n", "y_test_male = training_data['y_test_male']\n", "y_test_female = training_data['y_test_female']\n", "feature_cols = training_data['feature_cols']\n", "\n", "print('✓ Models loaded successfully!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 2: Generate Predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Male model predictions\n", "y_pred_male = male_model.predict(X_test_male)\n", "y_pred_proba_male = male_model.predict_proba(X_test_male)[:, 1]\n", "\n", "# Female model predictions\n", "y_pred_female = female_model.predict(X_test_female)\n", "y_pred_proba_female = female_model.predict_proba(X_test_female)[:, 1]\n", "\n", "print('✓ Predictions generated')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 3: Calculate Male Model Metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('═' * 60)\n", "print('MALE MODEL PERFORMANCE')\n", "print('═' * 60)\n", "\n", "male_accuracy = accuracy_score(y_test_male, y_pred_male)\n", "male_precision = precision_score(y_test_male, y_pred_male)\n", "male_recall = recall_score(y_test_male, y_pred_male)\n", "male_f1 = f1_score(y_test_male, y_pred_male)\n", "male_auc = roc_auc_score(y_test_male, y_pred_proba_male)\n", "\n", "print(f'Accuracy:   {male_accuracy:.4f} ({male_accuracy*100:.2f}%)')\n", "print(f'Precision:  {male_precision:.4f}')\n", "print(f'Recall:     {male_recall:.4f}')\n", "print(f'F1-Score:   {male_f1:.4f}')\n", "print(f'AUC-ROC:    {male_auc:.4f}')\n", "\n", "print('\\nDetailed Classification Report:')\n", "print(classification_report(y_test_male, y_pred_male))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 4: Calculate Female Model Metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('═' * 60)\n", "print('FEMALE MODEL PERFORMANCE')\n", "print('═' * 60)\n", "\n", "female_accuracy = accuracy_score(y_test_female, y_pred_female)\n", "female_precision = precision_score(y_test_female, y_pred_female)\n", "female_recall = recall_score(y_test_female, y_pred_female)\n", "female_f1 = f1_score(y_test_female, y_pred_female)\n", "female_auc = roc_auc_score(y_test_female, y_pred_proba_female)\n", "\n", "print(f'Accuracy:   {female_accuracy:.4f} ({female_accuracy*100:.2f}%)')\n", "print(f'Precision:  {female_precision:.4f}')\n", "print(f'Recall:     {female_recall:.4f}')\n", "print(f'F1-Score:   {female_f1:.4f}')\n", "print(f'AUC-ROC:    {female_auc:.4f}')\n", "\n", "print('\\nDetailed Classification Report:')\n", "print(classification_report(y_test_female, y_pred_female))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 5: Confusion Matrix Visualization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Male confusion matrix\n", "cm_male = confusion_matrix(y_test_male, y_pred_male)\n", "sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False)\n", "axes[0].set_title('Male Model - Confusion Matrix', fontweight='bold', fontsize=12)\n", "axes[0].set_xlabel('Predicted')\n", "axes[0].set_ylabel('Actual')\n", "\n", "# Female confusion matrix\n", "cm_female = confusion_matrix(y_test_female, y_pred_female)\n", "sns.heatmap(cm_female, annot=True, fmt='d', cmap='Oranges', ax=axes[1], cbar=False)\n", "axes[1].set_title('Female Model - Confusion Matrix', fontweight='bold', fontsize=12)\n", "axes[1].set_xlabel('Predicted')\n", "axes[1].set_ylabel('Actual')\n", "\n", "plt.tight_layout()\n", "plt.savefig('figures/confusion_matrices.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Confusion matrix visualization saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 6: ROC Curve Visualization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "\n", "# Male ROC curve\n", "fpr_male, tpr_male, _ = roc_curve(y_test_male, y_pred_proba_male)\n", "axes[0].plot(fpr_male, tpr_male, color='steelblue', lw=2, label=f'AUC = {male_auc:.3f}')\n", "axes[0].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n", "axes[0].set_xlabel('False Positive Rate')\n", "axes[0].set_ylabel('True Positive Rate')\n", "axes[0].set_title('Male Model - ROC Curve', fontweight='bold', fontsize=12)\n", "axes[0].legend()\n", "axes[0].grid(alpha=0.3)\n", "\n", "# Female ROC curve\n", "fpr_female, tpr_female, _ = roc_curve(y_test_female, y_pred_proba_female)\n", "axes[1].plot(fpr_female, tpr_female, color='salmon', lw=2, label=f'AUC = {female_auc:.3f}')\n", "axes[1].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n", "axes[1].set_xlabel('False Positive Rate')\n", "axes[1].set_ylabel('True Positive Rate')\n", "axes[1].set_title('Female Model - ROC Curve', fontweight='bold', fontsize=12)\n", "axes[1].legend()\n", "axes[1].grid(alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.savefig('figures/roc_curves.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ ROC curve visualization saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 7: Performance Comparison"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create comparison dataframe\n", "comparison_df = pd.DataFrame({\n", "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'],\n", "    'Male Model': [male_accuracy, male_precision, male_recall, male_f1, male_auc],\n", "    'Female Model': [female_accuracy, female_precision, female_recall, female_f1, female_auc]\n", "})\n", "\n", "print('═' * 60)\n", "print('PERFORMANCE COMPARISON')\n", "print('═' * 60)\n", "print(comparison_df.to_string(index=False))\n", "\n", "# Visualize\n", "fig, ax = plt.subplots(figsize=(10, 6))\n", "\n", "x = np.arange(len(comparison_df))\n", "width = 0.35\n", "\n", "bars1 = ax.bar(x - width/2, comparison_df['Male Model'], width, label='Male Model', color='steelblue')\n", "bars2 = ax.bar(x + width/2, comparison_df['Female Model'], width, label='Female Model', color='salmon')\n", "\n", "ax.set_ylabel('Score')\n", "ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)\n", "ax.set_xticks(x)\n", "ax.set_xticklabels(comparison_df['Metric'])\n", "ax.legend()\n", "ax.grid(alpha=0.3, axis='y')\n", "ax.set_ylim(0.7, 1.0)\n", "\n", "plt.tight_layout()\n", "plt.savefig('figures/performance_comparison.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('\\n✓ Performance comparison visualization saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 8: Feature Importance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Get feature importance from both models\n", "male_importance = pd.DataFrame({\n", "    'Feature': feature_cols,\n", "    'Importance': male_model.feature_importances_\n", "}).sort_values('Importance', ascending=False).head(10)\n", "\n", "female_importance = pd.DataFrame({\n", "    'Feature': feature_cols,\n", "    'Importance': female_model.feature_importances_\n", "}).sort_values('Importance', ascending=False).head(10)\n", "\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n", "\n", "# Male model feature importance\n", "axes[0].barh(male_importance['Feature'], male_importance['Importance'], color='steelblue')\n", "axes[0].set_title('Male Model - Top 10 Important Features', fontweight='bold')\n", "axes[0].set_xlabel('Importance')\n", "axes[0].invert_yaxis()\n", "\n", "# Female model feature importance\n", "axes[1].barh(female_importance['Feature'], female_importance['Importance'], color='salmon')\n", "axes[1].set_title('Female Model - Top 10 Important Features', fontweight='bold')\n", "axes[1].set_xlabel('Importance')\n", "axes[1].invert_yaxis()\n", "\n", "plt.tight_layout()\n", "plt.savefig('figures/feature_importance.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Feature importance visualization saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 9: Save Evaluation Results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save comparison to CSV\n", "comparison_df.to_csv('outputs/model_performance_comparison.csv', index=False)\n", "\n", "# Save feature importance\n", "male_importance.to_csv('outputs/male_feature_importance.csv', index=False)\n", "female_importance.to_csv('outputs/female_feature_importance.csv', index=False)\n", "\n", "print('═' * 60)\n", "print('EVALUATION COMPLETE')\n", "print('═' * 60)\n", "print('✓ All evaluation results saved!')\n", "print('\\nNext: Proceed to 06_SHAP_Explainability.ipynb for model interpretation.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Next Notebook\n", "\n", "Proceed to **06_SHAP_Explainability.ipynb** for detailed model interpretation using SHAP values."]}]}
