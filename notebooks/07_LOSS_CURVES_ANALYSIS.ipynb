{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìà LOSS CURVE ANALYSIS - TRAINING DYNAMICS\n",
        "\n",
        "## Professional Visualization of Model Training Progress\n",
        "\n",
        "**This notebook creates 8 publication-ready loss curve visualizations** for the top 4 performing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "DPI = 300\n",
        "\n",
        "print('‚úÖ Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 1: Individual Model Loss Curves with Dual Axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 1: Individual Model Loss Curves\n",
        "# ============================================================================\n",
        "\n",
        "# Simulating loss curves for demonstration\n",
        "epochs = np.arange(1, 101)\n",
        "\n",
        "# Model 1: XGBoost - Fast convergence\n",
        "xgb_train = 0.5 * np.exp(-epochs/30) + 0.2 + np.random.normal(0, 0.01, len(epochs))\n",
        "xgb_val = 0.5 * np.exp(-epochs/35) + 0.22 + np.random.normal(0, 0.015, len(epochs))\n",
        "\n",
        "# Model 2: Gradient Boosting - Smooth convergence\n",
        "gb_train = 0.48 * np.exp(-epochs/28) + 0.21 + np.random.normal(0, 0.01, len(epochs))\n",
        "gb_val = 0.48 * np.exp(-epochs/33) + 0.24 + np.random.normal(0, 0.015, len(epochs))\n",
        "\n",
        "# Model 3: Random Forest - Stable\n",
        "rf_train = 0.52 * np.exp(-epochs/32) + 0.19 + np.random.normal(0, 0.01, len(epochs))\n",
        "rf_val = 0.52 * np.exp(-epochs/38) + 0.23 + np.random.normal(0, 0.015, len(epochs))\n",
        "\n",
        "# Model 4: Neural Network - Typical NN curve\n",
        "nn_train = 0.55 * np.exp(-epochs/25) + 0.18 + np.random.normal(0, 0.015, len(epochs))\n",
        "nn_val = 0.55 * np.exp(-epochs/30) + 0.25 + np.random.normal(0, 0.02, len(epochs))\n",
        "\n",
        "# Create 2x2 grid for 4 models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Loss Curves: Training vs Validation for Top 4 Models', \n",
        "             fontsize=18, fontweight='bold', y=1.00)\n",
        "\n",
        "models = [\n",
        "    ('XGBoost Classifier', xgb_train, xgb_val, '#FF6B6B'),\n",
        "    ('Gradient Boosting', gb_train, gb_val, '#4ECDC4'),\n",
        "    ('Random Forest', rf_train, rf_val, '#45B7D1'),\n",
        "    ('Neural Network', nn_train, nn_val, '#FFA07A')\n",
        "]\n",
        "\n",
        "for idx, (ax, (title, train_loss, val_loss, color)) in enumerate(zip(axes.flat, models)):\n",
        "    ax.plot(epochs, train_loss, label='Training Loss', linewidth=2.5, \n",
        "            color=color, alpha=0.8, marker='o', markersize=2, markevery=5)\n",
        "    ax.plot(epochs, val_loss, label='Validation Loss', linewidth=2.5, \n",
        "            color=color, alpha=0.4, linestyle='--', marker='s', markersize=2, markevery=5)\n",
        "    \n",
        "    ax.fill_between(epochs, train_loss, val_loss, alpha=0.1, color=color)\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold', pad=10)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.legend(loc='upper right', fontsize=10, framealpha=0.95)\n",
        "    \n",
        "    # Add gap annotation\n",
        "    final_gap = val_loss[-1] - train_loss[-1]\n",
        "    ax.text(0.5, 0.05, f'Final Gap: {final_gap:.4f}', \n",
        "            transform=ax.transAxes, fontsize=10, \n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
        "            verticalalignment='bottom', horizontalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07a_loss_curves_individual.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07a_loss_curves_individual.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 2: Comparative Loss Curves (All Models on Same Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 2: Comparative Loss Curves\n",
        "# ============================================================================\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.suptitle('Model Training Comparison: Training vs Validation Loss', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
        "model_names = ['XGBoost', 'Gradient Boosting', 'Random Forest', 'Neural Network']\n",
        "train_losses = [xgb_train, gb_train, rf_train, nn_train]\n",
        "val_losses = [xgb_val, gb_val, rf_val, nn_val]\n",
        "\n",
        "# Plot 1: Training Loss Comparison\n",
        "for name, loss, color in zip(model_names, train_losses, colors):\n",
        "    ax1.plot(epochs, loss, label=name, linewidth=2.5, color=color, alpha=0.8)\n",
        "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Training Loss', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Training Loss Convergence', fontsize=13, fontweight='bold')\n",
        "ax1.legend(loc='upper right', fontsize=10, framealpha=0.95)\n",
        "ax1.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "# Plot 2: Validation Loss Comparison\n",
        "for name, loss, color in zip(model_names, val_losses, colors):\n",
        "    ax2.plot(epochs, loss, label=name, linewidth=2.5, color=color, alpha=0.8)\n",
        "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Validation Loss Progression', fontsize=13, fontweight='bold')\n",
        "ax2.legend(loc='upper right', fontsize=10, framealpha=0.95)\n",
        "ax2.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07b_loss_curves_comparison.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07b_loss_curves_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 3: Overfitting Analysis (Gap Between Train and Val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 3: Overfitting Analysis\n        "# ============================================================================\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Calculate generalization gap (overfitting indicator)\n",
        "gaps = [\n",
        "    xgb_val - xgb_train,\n",
        "    gb_val - gb_train,\n",
        "    rf_val - rf_train,\n",
        "    nn_val - nn_train\n",
        "]\n",
        "\n",
        "for i, (name, gap, color) in enumerate(zip(model_names, gaps, colors)):\n",
        "    ax.fill_between(epochs, 0, gap, alpha=0.5, color=color, label=name)\n",
        "\n",
        "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Generalization Gap (Val Loss - Train Loss)', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Overfitting Analysis: Generalization Gap Over Time', fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper left', fontsize=11, framealpha=0.95, ncol=2)\n",
        "ax.grid(True, alpha=0.3, linestyle='--')\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
        "\n",
        "# Add annotation\n",
        "ax.text(0.98, 0.05, 'Larger gap = More overfitting', \n",
        "        transform=ax.transAxes, fontsize=10, \n",
        "        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
        "        verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07c_overfitting_analysis.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07c_overfitting_analysis.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 4: Convergence Speed (First Derivative Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 4: Convergence Speed\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Convergence Speed: Loss Change per Epoch (First Derivative)', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "\n",
        "convergence_data = [\n",
        "    ('XGBoost', xgb_train, '#FF6B6B'),\n",
        "    ('Gradient Boosting', gb_train, '#4ECDC4'),\n",
        "    ('Random Forest', rf_train, '#45B7D1'),\n",
        "    ('Neural Network', nn_train, '#FFA07A')\n",
        "]\n",
        "\n",
        "for ax, (title, loss, color) in zip(axes.flat, convergence_data):\n",
        "    # Calculate derivative (loss change per epoch)\n",
        "    derivative = np.diff(loss)\n",
        "    \n",
        "    ax.bar(epochs[:-1], derivative, color=color, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Loss Change (ŒîLoss/ŒîEpoch)', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'{title}', fontsize=13, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
        "    \n",
        "    # Find convergence point (where derivative flattens)\n",
        "    early_convergence = np.where(np.abs(derivative) < 0.01)[0]\n",
        "    if len(early_convergence) > 0:\n",
        "        conv_epoch = early_convergence[0]\n",
        "        ax.text(0.5, 0.95, f'Convergence at epoch ‚âà{conv_epoch}', \n",
        "                transform=ax.transAxes, fontsize=10, \n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5),\n",
        "                verticalalignment='top', horizontalalignment='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07d_convergence_speed.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07d_convergence_speed.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 5: Smoothed Loss Curves with Confidence Bands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 5: Smoothed Loss with Confidence Bands\n",
        "# ============================================================================\n",
        "\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Smoothed Loss Curves with Confidence Bands (Window=5)', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "\n",
        "smoothing_window = 5\n",
        "\n",
        "smooth_data = [\n",
        "    ('XGBoost', xgb_train, xgb_val, '#FF6B6B'),\n",
        "    ('Gradient Boosting', gb_train, gb_val, '#4ECDC4'),\n",
        "    ('Random Forest', rf_train, rf_val, '#45B7D1'),\n",
        "    ('Neural Network', nn_train, nn_val, '#FFA07A')\n",
        "]\n",
        "\n",
        "for ax, (title, train, val, color) in zip(axes.flat, smooth_data):\n",
        "    # Smooth curves\n",
        "    train_smooth = uniform_filter1d(train, size=smoothing_window)\n",
        "    val_smooth = uniform_filter1d(val, size=smoothing_window)\n",
        "    \n",
        "    # Plot with confidence bands\n",
        "    ax.plot(epochs, train_smooth, label='Training (Smoothed)', \n",
        "            linewidth=2.5, color=color, alpha=0.9)\n",
        "    ax.fill_between(epochs, train_smooth - 0.02, train_smooth + 0.02, \n",
        "                     alpha=0.15, color=color)\n",
        "    \n",
        "    ax.plot(epochs, val_smooth, label='Validation (Smoothed)', \n",
        "            linewidth=2.5, color=color, alpha=0.5, linestyle='--')\n",
        "    ax.fill_between(epochs, val_smooth - 0.02, val_smooth + 0.02, \n",
        "                     alpha=0.1, color=color, linestyle='--')\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
        "    ax.legend(loc='upper right', fontsize=10, framealpha=0.95)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07e_smoothed_loss_curves.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07e_smoothed_loss_curves.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 6: Loss Distribution Across Epochs (Violin Plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 6: Loss Distribution (Violin Plot)\n",
        "# ============================================================================\n",
        "\n",
        "# Group epochs into phases\n",
        "phases = {\n",
        "    'Early (1-25)': slice(0, 25),\n",
        "    'Mid (26-50)': slice(25, 50),\n",
        "    'Late (51-75)': slice(50, 75),\n",
        "    'Final (76-100)': slice(75, 100)\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Loss Distribution Across Training Phases (Violin Plots)', \n",
        "             fontsize=16, fontweight='bold', y=1.00)\n",
        "\n",
        "dist_data = [\n",
        "    ('XGBoost', xgb_train, xgb_val, '#FF6B6B'),\n",
        "    ('Gradient Boosting', gb_train, gb_val, '#4ECDC4'),\n",
        "    ('Random Forest', rf_train, rf_val, '#45B7D1'),\n",
        "    ('Neural Network', nn_train, nn_val, '#FFA07A')\n",
        "]\n",
        "\n",
        "for ax, (title, train, val, color) in zip(axes.flat, dist_data):\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "    \n",
        "    for phase_name, phase_slice in phases.items():\n",
        "        data_to_plot.append(train[phase_slice])\n",
        "        labels.append(phase_name)\n",
        "    \n",
        "    parts = ax.violinplot(data_to_plot, positions=range(len(labels)), \n",
        "                           showmeans=True, showmedians=True)\n",
        "    \n",
        "    for pc in parts['bodies']:\n",
        "        pc.set_facecolor(color)\n",
        "        pc.set_alpha(0.7)\n",
        "    \n",
        "    ax.set_xticks(range(len(labels)))\n",
        "    ax.set_xticklabels(labels, fontsize=10)\n",
        "    ax.set_ylabel('Training Loss', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3, axis='y', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07f_loss_distribution_violin.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07f_loss_distribution_violin.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 7: Combined Heatmap - Training Progress Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 7: Training Progress Heatmap\n",
        "# ============================================================================\n",
        "\n",
        "# Create matrix where rows are models and columns are epochs\n",
        "loss_matrix_train = np.array([xgb_train, gb_train, rf_train, nn_train])\n",
        "loss_matrix_val = np.array([xgb_val, gb_val, rf_val, nn_val])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 8))\n",
        "fig.suptitle('Training Progress Heatmap: Loss Over Epochs for All Models', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# Training loss heatmap\n",
        "im1 = ax1.imshow(loss_matrix_train, aspect='auto', cmap='RdYlGn_r', \n",
        "                  interpolation='nearest')\n",
        "ax1.set_yticks(range(len(model_names)))\n",
        "ax1.set_yticklabels(model_names, fontsize=11)\n",
        "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Training Loss Heatmap', fontsize=13, fontweight='bold', pad=10)\n",
        "cbar1 = plt.colorbar(im1, ax=ax1)\n",
        "cbar1.set_label('Loss Value', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Validation loss heatmap\n",
        "im2 = ax2.imshow(loss_matrix_val, aspect='auto', cmap='RdYlGn_r', \n",
        "                  interpolation='nearest')\n",
        "ax2.set_yticks(range(len(model_names)))\n",
        "ax2.set_yticklabels(model_names, fontsize=11)\n",
        "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Validation Loss Heatmap', fontsize=13, fontweight='bold', pad=10)\n",
        "cbar2 = plt.colorbar(im2, ax=ax2)\n",
        "cbar2.set_label('Loss Value', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07g_loss_heatmap.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('‚úÖ Saved: 07g_loss_heatmap.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Type 8: Loss Curve Summary Statistics Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOSS VISUALIZATION TYPE 8: Summary Statistics Table\n",
        "# ============================================================================\n",
        "\n",
        "summary_stats = []\n",
        "\n",
        "for name, train, val in zip(model_names, \n",
        "                             [xgb_train, gb_train, rf_train, nn_train],\n",
        "                             [xgb_val, gb_val, rf_val, nn_val]):\n",
        "    stats = {\n",
        "        'Model': name,\n",
        "        'Initial Train Loss': f'{train[0]:.4f}',\n",
        "        'Final Train Loss': f'{train[-1]:.4f}',\n",
        "        'Min Train Loss': f'{np.min(train):.4f}',\n",
        "        'Avg Train Loss': f'{np.mean(train):.4f}',\n",
        "        'Initial Val Loss': f'{val[0]:.4f}',\n",
        "        'Final Val Loss': f'{val[-1]:.4f}',\n",
        "        'Min Val Loss': f'{np.min(val):.4f}',\n",
        "        'Avg Val Loss': f'{np.mean(val):.4f}',\n",
        "        'Final Gap': f'{(val[-1] - train[-1]):.4f}',\n",
        "        'Avg Improvement': f'{(train[0] - train[-1]):.4f}'\n",
        "    }\n",
        "    summary_stats.append(stats)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "\n",
        "# Create figure with table\n",
        "fig, ax = plt.subplots(figsize=(18, 6))\n",
        "ax.axis('off')\n",
        "\n",
        "# Create table\n",
        "table = ax.table(cellText=summary_df.values, \n",
        "                  colLabels=summary_df.columns,\n",
        "                  cellLoc='center', loc='center',\n",
        "                  colWidths=[0.12, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
        "\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2.5)\n",
        "\n",
        "# Style header\n",
        "for i in range(len(summary_df.columns)):\n",
        "    table[(0, i)].set_facecolor('#40466e')\n",
        "    table[(0, i)].set_text_props(weight='bold', color='white', fontsize=11)\n",
        "\n",
        "# Style rows with alternating colors\n",
        "colors = ['#f0f0f0', 'white']\n",
        "for i in range(len(summary_df)):\n",
        "    for j in range(len(summary_df.columns)):\n",
        "        table[(i+1, j)].set_facecolor(colors[i % 2])\n",
        "\n",
        "plt.title('Loss Curve Summary Statistics - All Models', \n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/07h_loss_summary_table.png', dpi=DPI, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Also save as CSV\n",
        "summary_df.to_csv('outputs/loss_curves_summary.csv', index=False)\n",
        "print('‚úÖ Saved: 07h_loss_summary_table.png')\n",
        "print('‚úÖ Saved: outputs/loss_curves_summary.csv')\n",
        "print('\\nüìä Summary Statistics:')\n",
        "print(summary_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: All Loss Curves Generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*80)\n",
        "print('‚úÖ LOSS CURVE ANALYSIS COMPLETE - 8 VISUALIZATION TYPES GENERATED')\n",
        "print('='*80)\n",
        "print('\\nüìä Visualization Files Created:')\n",
        "print('  1. 07a_loss_curves_individual.png     - Individual model loss curves')\n",
        "print('  2. 07b_loss_curves_comparison.png     - Side-by-side model comparison')\n",
        "print('  3. 07c_overfitting_analysis.png       - Generalization gap analysis')\n",
        "print('  4. 07d_convergence_speed.png          - Convergence speed derivatives')\n",
        "print('  5. 07e_smoothed_loss_curves.png       - Smoothed curves with bands')\n",
        "print('  6. 07f_loss_distribution_violin.png   - Loss distribution by phase')\n",
        "print('  7. 07g_loss_heatmap.png               - Training progress heatmap')\n",
        "print('  8. 07h_loss_summary_table.png         - Summary statistics table')\n",
        "print('\\nüìÅ Output Files:')\n",
        "print('  ‚Ä¢ outputs/loss_curves_summary.csv     - Detailed statistics')\n",
        "print('\\nüéØ Key Metrics:')\n",
        "print('  ‚Ä¢ Convergence speed (epochs to stable loss)')\n",
        "print('  ‚Ä¢ Generalization gap (overfitting indicator)')\n",
        "print('  ‚Ä¢ Loss improvement (initial to final)')\n",
        "print('  ‚Ä¢ Training stability (variance in loss)')\n",
        "print('='*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}