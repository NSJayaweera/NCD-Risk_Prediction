{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# SHAP Explainability Analysis\n", "\n", "Generate SHAP (SHapley Additive exPlanations) values for model interpretation and transparency."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 1: Load Models and Data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import joblib\n", "import shap\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "# Load training data\n", "training_data = joblib.load('models/training_data.pkl')\n", "\n", "male_model = training_data['male_model']\n", "female_model = training_data['female_model']\n", "X_test_male = training_data['X_test_male']\n", "X_test_female = training_data['X_test_female']\n", "feature_cols = training_data['feature_cols']\n", "\n", "print('✓ Models and data loaded successfully!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 2: Calculate SHAP Values for Male Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('═' * 60)\n", "print('MALE MODEL - SHAP EXPLAINABILITY')\n", "print('═' * 60)\n", "\n", "print('\\nCalculating SHAP values for male model...')\n", "\n", "# Create explainer\n", "explainer_male = shap.TreeExplainer(male_model)\n", "shap_values_male = explainer_male.shap_values(X_test_male)\n", "\n", "print('✓ SHAP values calculated!')\n", "print(f'SHAP values shape: {shap_values_male.shape}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 3: Calculate SHAP Values for Female Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('═' * 60)\n", "print('FEMALE MODEL - SHAP EXPLAINABILITY')\n", "print('═' * 60)\n", "\n", "print('\\nCalculating SHAP values for female model...')\n", "\n", "# Create explainer\n", "explainer_female = shap.TreeExplainer(female_model)\n", "shap_values_female = explainer_female.shap_values(X_test_female)\n", "\n", "print('✓ SHAP values calculated!')\n", "print(f'SHAP values shape: {shap_values_female.shape}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 4: Summary Plot - Male Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "shap.summary_plot(shap_values_male, X_test_male, feature_names=feature_cols, show=False)\n", "plt.title('Male Model - SHAP Summary Plot', fontweight='bold', fontsize=14)\n", "plt.tight_layout()\n", "plt.savefig('figures/shap_summary_male.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Male model SHAP summary plot saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 5: Summary Plot - Female Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "shap.summary_plot(shap_values_female, X_test_female, feature_names=feature_cols, show=False)\n", "plt.title('Female Model - SHAP Summary Plot', fontweight='bold', fontsize=14)\n", "plt.tight_layout()\n", "plt.savefig('figures/shap_summary_female.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Female model SHAP summary plot saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 6: Bar Plot - Feature Importance (Average SHAP)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n", "\n", "# Male model\n", "shap.summary_plot(shap_values_male, X_test_male, feature_names=feature_cols, plot_type='bar', show=False)\n", "axes[0].set_title('Male Model - Feature Importance', fontweight='bold', fontsize=12)\n", "axes[0].set_xlabel('Mean |SHAP value|')\n", "\n", "# Female model\n", "shap.summary_plot(shap_values_female, X_test_female, feature_names=feature_cols, plot_type='bar', show=False)\n", "axes[1].set_title('Female Model - Feature Importance', fontweight='bold', fontsize=12)\n", "axes[1].set_xlabel('Mean |SHAP value|')\n", "\n", "plt.tight_layout()\n", "plt.savefig('figures/shap_bar_importance.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ SHAP bar plots saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 7: Force Plot - Sample Prediction (Male)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Show force plot for first sample\n", "sample_idx = 0\n", "\n", "plt.figure(figsize=(20, 3))\n", "shap.force_plot(explainer_male.expected_value, \n", "                shap_values_male[sample_idx:sample_idx+1], \n", "                X_test_male.iloc[sample_idx:sample_idx+1], \n", "                feature_names=feature_cols,\n", "                show=False,\n", "                matplotlib=True)\n", "plt.title('Male Model - Force Plot (Sample 1)', fontweight='bold', fontsize=12)\n", "plt.tight_layout()\n", "plt.savefig('figures/shap_force_male.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Male model force plot saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 8: Force Plot - Sample Prediction (Female)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Show force plot for first sample\n", "sample_idx = 0\n", "\n", "plt.figure(figsize=(20, 3))\n", "shap.force_plot(explainer_female.expected_value, \n", "                shap_values_female[sample_idx:sample_idx+1], \n", "                X_test_female.iloc[sample_idx:sample_idx+1], \n", "                feature_names=feature_cols,\n", "                show=False,\n", "                matplotlib=True)\n", "plt.title('Female Model - Force Plot (Sample 1)', fontweight='bold', fontsize=12)\n", "plt.tight_layout()\n", "plt.savefig('figures/shap_force_female.png', dpi=300, bbox_inches='tight')\n", "plt.show()\n", "print('✓ Female model force plot saved!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Step 9: Save SHAP Explainers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Save explainers for later use\n", "joblib.dump({\n", "    'explainer_male': explainer_male,\n", "    'explainer_female': explainer_female,\n", "    'shap_values_male': shap_values_male,\n", "    'shap_values_female': shap_values_female,\n", "    'feature_cols': feature_cols\n", "}, 'models/shap_explainers.pkl')\n", "\n", "print('═' * 60)\n", "print('EXPLAINABILITY ANALYSIS COMPLETE')\n", "print('═' * 60)\n", "print('✓ All SHAP visualizations and explainers saved!')\n", "print('\\nThe models are now fully trained and interpretable!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Key Takeaways\n", "\n", "SHAP analysis helps us understand:\n", "1. **Feature Importance**: Which factors most influence predictions\n", "2. **Feature Impact**: How each feature contributes (positively/negatively)\n", "3. **Individual Predictions**: Why the model made specific predictions\n", "4. **Model Transparency**: Making the 'black box' interpretable\n", "\n", "This is critical for healthcare applications where explainability is essential!"]}]}
