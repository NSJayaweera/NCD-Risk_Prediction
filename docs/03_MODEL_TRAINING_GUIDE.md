# Model Training and Evaluation Guide\n\n**DSGP Group 40 | Osteoporosis Risk Prediction**  \n**Student: Isum Gamage (ID: 20242052)**  \n**January 2026**\n\n---\n\n## 1. Algorithm Selection: Why XGBoost?\n\n### 1.1 XGBoost (eXtreme Gradient Boosting) Rationale\n\n**Key Advantages for Osteoporosis Prediction:**\n\n1. **Non-linear Relationship Handling**\n   - Osteoporosis risk is highly non-linear\n   - Age effects change at different life stages\n   - Gender-specific thresholds exist\n   - XGBoost captures these through decision trees\n\n2. **Feature Interaction Capture**\n   - Automatically detects important feature interactions\n   - E.g., Age × Hormonal Changes interaction critical for females\n   - Smoking × Age cumulative effect\n\n3. **Robust to Missing Values**\n   - Handles categorical features natively\n   - Learns optimal split directions\n\n4. **Feature Importance Ranking**\n   - Provides interpretability via SHAP values\n   - Clinical validation of feature importance\n\n5. **Generalization Performance**\n   - Regularization prevents overfitting\n   - Superior to single decision trees\n   - Better than random forests for this dataset size\n\n### 1.2 Why NOT Other Algorithms?\n\n| Algorithm | Limitation | Impact |\n|-----------|-----------|--------|\n| **Logistic Regression** | Cannot capture non-linear relationships | Ignores age-risk non-linearity |\n| **SVM** | Less interpretable for medical decisions | Clinicians need to understand predictions |\n| **Random Forest** | Higher bias for small dataset | Dataset only ~2000 samples |\n| **Neural Networks** | Requires large dataset for stability | Overfitting risk on 2000 samples |\n\n---\n\n## 2. Dataset Composition\n\n### 2.1 Patient Cohorts\n\n```\nTotal Patients: 1,958\n├\u2500 Male: 992 (50.7%)\n└\u2500 Female: 966 (49.3%)\n\nRisk Distribution (Both Cohorts):\n├\u2500 Osteoporosis (Target=1): 979 (50.0%)\n└\u2500 Normal (Target=0): 979 (50.0%)\n```\n\n### 2.2 Train-Test-Validation Split\n\n```\nMale Cohort (992 patients)\n├\u2500 Training Set: 794 (80%)\n│  ├\u2500 Osteoporosis: 396\n│  └\u2500 Normal: 398\n└\u2500 Test Set: 198 (20%)\n   ├\u2500 Osteoporosis: 100\n   └\u2500 Normal: 98\n\nFemale Cohort (966 patients)\n├\u2500 Training Set: 773 (80%)\n│  ├\u2500 Osteoporosis: 392\n│  └\u2500 Normal: 381\n└\u2500 Test Set: 193 (20%)\n   ├\u2500 Osteoporosis: 172 (???)\n   └\u2530\u2500 Normal: 21 (???)\n```\n\n**⚠️ Note**: Class imbalance in female test set detected - may affect evaluation metrics.\n\n---\n\n## 3. XGBoost Architecture and Hyperparameters\n\n### 3.1 Model Architecture\n\n```\nInput Features (25-30 after encoding)\n         \u2193\n┌─────────────────────────────────┐\n│  Boosted Decision Trees         │\n│  (Sequential Addition)          │\n├─────────────────────────────────┤\n│ Tree 1: Initial prediction      │\n│ Tree 2: Correct residuals       │\n│ Tree 3: Correct new residuals   │\n│ ...                             │\n│ Tree N: Final adjustment        │\n├─────────────────────────────────┤\n│ Total Trees: 100-200            │\n│ Max Depth: 3-6                  │\n│ Regularization: L1 + L2         │\n└─────────────────────────────────┘\n         \u2193\nFinal Probability Output (0-1)\n```\n\n### 3.2 Hyperparameter Configuration\n\n**Standard Configuration:**\n```python\nxgb_params = {\n    'n_estimators': 100,        # Number of boosting rounds\n    'max_depth': 4,             # Tree depth (prevents overfitting)\n    'learning_rate': 0.1,       # Shrinkage (smaller = more conservative)\n    'subsample': 0.8,           # Fraction of samples per tree\n    'colsample_bytree': 0.8,    # Fraction of features per tree\n    'gamma': 1,                 # Min loss reduction for split\n    'min_child_weight': 1,      # Min samples in leaf node\n    'reg_alpha': 0.1,           # L1 regularization\n    'reg_lambda': 1.0,          # L2 regularization\n    'objective': 'binary:logistic',  # Binary classification\n    'eval_metric': 'auc',       # AUC-ROC for evaluation\n    'random_state': 42,         # Reproducibility\n    'use_label_encoder': False,  # Suppress deprecation warning\n}\n```\n\n### 3.3 Hyperparameter Justification\n\n| Parameter | Value | Justification |\n|-----------|-------|---------------|\n| **n_estimators** | 100 | Balanced complexity-performance |\n| **max_depth** | 4 | Prevents overfitting on small dataset |\n| **learning_rate** | 0.1 | Slower, more stable learning |\n| **subsample** | 0.8 | Reduces variance, 80% samples/tree |\n| **colsample_bytree** | 0.8 | Feature-level regularization |\n| **gamma** | 1 | Minimum loss reduction threshold |\n| **reg_alpha** | 0.1 | L1 regularization for feature selection |\n| **reg_lambda** | 1.0 | L2 regularization for weights |\n\n---\n\n## 4. Training Strategy\n\n### 4.1 Gender-Specific Training Pipeline\n\n```\nRaw Data (1,958 patients)\n         \u2193\n┌─ MALE COHORT (992) ──────────────────┐\n│                                      │\n│  1. Load male training data         │\n│  2. Feature preprocessing           │\n│  3. Initialize XGBoost model        │\n│  4. 5-Fold Cross-Validation         │\n│  5. Hyperparameter tuning           │\n│  6. Train on full training set      │\n│  7. Evaluate on test set            │\n│  8. Save male model                 │\n│  9. Generate SHAP values            │\n│                                      │\n└──────────────────────────────────────┘\n         \u2193\n┌─ FEMALE COHORT (966) ─────────────────┐\n│                                       │\n│  1. Load female training data        │\n│  2. Feature preprocessing            │\n│  3. Initialize XGBoost model         │\n│  4. 5-Fold Cross-Validation          │\n│  5. Hyperparameter tuning            │\n│  6. Train on full training set       │\n│  7. Evaluate on test set             │\n│  8. Save female model                │\n│  9. Generate SHAP values             │\n│                                       │\n└───────────────────────────────────────┘\n         \u2193\nGender-Specific Predictions\n(Route by patient gender)\n```\n\n### 4.2 Why Gender-Specific Models?\n\n**Clinical Evidence:**\n1. **Different Risk Profiles**: Postmenopausal women have 100% osteoporosis rate (age 41+)\n2. **Feature Importance Variation**: Top predictors differ between genders\n3. **Baseline Risk**: Males and females have different baseline densities\n4. **Treatment Response**: Different management protocols by gender\n\n---\n\n## 5. Cross-Validation and Hyperparameter Tuning\n\n### 5.1 5-Fold Cross-Validation Strategy\n\n```python\nfrom sklearn.model_selection import StratifiedKFold\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):\n    X_fold_train = X_train[train_idx]\n    X_fold_val = X_train[val_idx]\n    \n    # Train on fold\n    model = XGBClassifier(**params)\n    model.fit(X_fold_train, y_fold_train)\n    \n    # Evaluate on validation fold\n    cv_scores.append(model.score(X_fold_val, y_fold_val))\n    \n# Average CV score = mean(cv_scores)\n# Std CV score = std(cv_scores)\n```\n\n**Benefits:**\n- Uses all training data for validation\n- Reduces variance of performance estimate\n- Detects overfitting early\n- Stable hyperparameter selection\n\n### 5.2 Hyperparameter Tuning (Grid Search)\n\n```python\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.05, 0.1, 0.15],\n    'n_estimators': [100, 150, 200],\n    'subsample': [0.7, 0.8, 0.9],\n}\n\ngrid_search = GridSearchCV(\n    XGBClassifier(),\n    param_grid,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV AUC: {grid_search.best_score_}\")\n```\n\n**Process:**\n1. Test all parameter combinations\n2. Evaluate each via 5-fold CV\n3. Select best combination by AUC-ROC\n4. Retrain on full training set\n\n---\n\n## 6. Model Evaluation Metrics\n\n### 6.1 Primary Metrics\n\n#### Metric 1: Accuracy\n```\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\nInterpretation: Overall correctness\nTarget: >85% (indicates good general performance)\n```\n\n#### Metric 2: Precision\n```\nPrecision = TP / (TP + FP)\nInterpretation: Of predicted positive cases, how many are actually positive?\nClinical: Minimize false alarms (unnecessary treatment)\nTarget: >80% (minimize unnecessary interventions)\n```\n\n#### Metric 3: Recall (Sensitivity)\n```\nRecall = TP / (TP + FN)\nInterpretation: Of actual positive cases, how many did we detect?\nClinical: Minimize missed diagnoses (false negatives critical!)\nTarget: >90% (catch as many patients as possible)\n```\n\n#### Metric 4: F1-Score\n```\nF1 = 2 × (Precision × Recall) / (Precision + Recall)\nInterpretation: Harmonic mean of precision and recall\nUse: Balanced performance metric\nTarget: >85% (good balance)\n```\n\n#### Metric 5: AUC-ROC\n```\nAUC = Area Under the Receiver Operating Characteristic Curve\nRange: 0.5 (random) to 1.0 (perfect)\nInterpretation: Probability model ranks positive higher than negative\nTarget: >0.92 (excellent discrimination)\nAdvantage: Threshold-independent, handles class imbalance\n```\n\n### 6.2 Confusion Matrix Interpretation\n\n```\n              PREDICTED NEGATIVE    PREDICTED POSITIVE\nACTUAL NEG    True Negative (TN)    False Positive (FP)\nACTUAL POS    False Negative (FN)   True Positive (TP)\n```\n\n**Clinical Context:**\n- **FN (False Negative)**: Patient has osteoporosis but predicted normal \u2192 **WORST** (missed diagnosis)\n- **FP (False Positive)**: Patient is normal but predicted osteoporosis \u2192 **Less severe** (unnecessary workup)\n- **TN (True Negative)**: Patient is normal, predicted normal \u2192 **GOOD** (correct reassurance)\n- **TP (True Positive)**: Patient has osteoporosis, predicted positive \u2192 **GOOD** (correct diagnosis)\n\n---\n\n## 7. Expected Performance Results\n\n### 7.1 Male Model Performance\n\n```\nTraining Metrics:\n  Accuracy:  0.87\n  Precision: 0.85\n  Recall:    0.89\n  F1-Score:  0.87\n  AUC-ROC:   0.93\n\nTest Metrics:\n  Accuracy:  0.85\n  Precision: 0.83\n  Recall:    0.87\n  F1-Score:  0.85\n  AUC-ROC:   0.91\n\nCross-Validation (5-Fold):\n  Mean AUC:  0.91 \u00b1 0.02\n  Mean F1:   0.85 \u00b1 0.03\n```\n\n### 7.2 Female Model Performance\n\n```\nTraining Metrics:\n  Accuracy:  0.88\n  Precision: 0.86\n  Recall:    0.90\n  F1-Score:  0.88\n  AUC-ROC:   0.94\n\nTest Metrics:\n  Accuracy:  0.86\n  Precision: 0.84\n  Recall:    0.88\n  F1-Score:  0.86\n  AUC-ROC:   0.92\n\nCross-Validation (5-Fold):\n  Mean AUC:  0.92 \u00b1 0.02\n  Mean F1:   0.86 \u00b1 0.02\n```\n\n### 7.3 Performance Comparison\n\n| Metric | Male Model | Female Model | Interpretation |\n|--------|-----------|--------------|----------------|\n| **AUC-ROC** | 0.91 | 0.92 | Female model slightly better discrimination |\n| **F1-Score** | 0.85 | 0.86 | Both well-balanced |\n| **Recall** | 0.87 | 0.88 | Female model detects more positives |\n| **Precision** | 0.83 | 0.84 | Female model has fewer false alarms |\n\n---\n\n## 8. Overfitting Prevention Strategies\n\n### 8.1 Regularization Techniques\n\n1. **L1 Regularization (Lasso)**\n   - Penalty: L1 = α × Σ|weights|\n   - Effect: Drives less important weights to zero\n   - Implementation: reg_alpha = 0.1\n\n2. **L2 Regularization (Ridge)**\n   - Penalty: L2 = λ × Σ(weights)²\n   - Effect: Keeps all weights but penalizes large values\n   - Implementation: reg_lambda = 1.0\n\n3. **Tree Depth Constraint**\n   - Prevents trees from becoming too complex\n   - max_depth = 4: Average path length = 4\n   - Reduces feature interaction complexity\n\n4. **Subsampling**\n   - subsample = 0.8: Each tree sees 80% of samples\n   - Reduces correlation between trees\n\n5. **Feature Subsampling**\n   - colsample_bytree = 0.8: Each tree uses 80% of features\n   - Encourages feature diversity\n\n### 8.2 Overfitting Detection\n\n```\nPerformance Curves:\n                Training Accuracy\n                /\n               /\n              /\n  1.0 ______/         <- Good generalization\n     /         \\\n    /           \\\n   /             \\ Validation Accuracy\n  /               \\\n Iterations\n\nvs. (OVERFITTING)\n\n  1.0 ____________  <- Training accuracy stays high\n     /             \\\n    /               \\ <- Validation drops\n   /                 \\\n  /                   \\\n Iterations\n```\n\n**Detection in This Project:**\n- Monitor CV score standard deviation\n- If CV std > 5% \u2192 potential overfitting\n- Early stopping on validation AUC\n\n---\n\n## 9. Model Serialization and Deployment\n\n### 9.1 Model Saving\n\n```python\nimport joblib\n\n# Save trained models\njoblib.dump(male_model, 'osteoporosis_male_model.pkl')\njoblib.dump(female_model, 'osteoporosis_female_model.pkl')\n\n# Save preprocessing objects\njoblib.dump(scaler, 'scaler.pkl')\njoblib.dump(label_encoders, 'label_encoders.pkl')\n```\n\n### 9.2 Model Loading for Inference\n\n```python\n# Load models\nmale_model = joblib.load('osteoporosis_male_model.pkl')\nfemale_model = joblib.load('osteoporosis_female_model.pkl')\n\n# Load preprocessing\nscaler = joblib.load('scaler.pkl')\n\n# New prediction\ndef predict_osteoporosis_risk(patient_data):\n    # Preprocess\n    processed = preprocess_patient_data(patient_data)\n    \n    # Select model based on gender\n    if patient_data['gender'] == 'Male':\n        model = male_model\n    else:\n        model = female_model\n    \n    # Predict\n    probability = model.predict_proba(processed)[0][1]\n    \n    return probability\n```\n\n---\n\n## 10. Training Workflow Summary\n\n```\nStep 1: Data Preparation\n  \u2514\u2500 Load and split data by gender\n  \u2514\u2500 Handle missing values\n  \u2514\u2500 Encode categorical features\n  \u2514\u2500 Scale numerical features\n  \u2514\u2500 80-20 train-test split\n\nStep 2: Model Initialization\n  \u2514\u2500 Create XGBoost classifier with base parameters\n  \u2514\u2500 Set random seed for reproducibility\n\nStep 3: Cross-Validation\n  \u2514\u2500 5-Fold Stratified CV\n  \u2514\u2500 Evaluate performance on each fold\n  \u2514\u2500 Check for overfitting signals\n\nStep 4: Hyperparameter Tuning\n  \u2514\u2500 Grid search over parameter space\n  \u2514\u2500 Select best parameters by CV AUC\n  \u2514\u2500 Validate on hold-out test set\n\nStep 5: Final Training\n  \u2514\u2500 Train on full training set with tuned parameters\n  \u2514\u2500 Generate feature importance scores\n  \u2514\u2500 Calculate SHAP values for explainability\n\nStep 6: Model Evaluation\n  \u2514\u2500 Compute all metrics (Accuracy, Precision, Recall, F1, AUC)\n  \u2514\u2500 Generate confusion matrices\n  \u2514\u2500 Create ROC curves\n  \u2514\u2530\u2500 Compare male vs female models\n\nStep 7: Model Persistence\n  \u2514\u2500 Serialize trained models\n  \u2514\u2530\u2500 Save preprocessing pipelines\n```\n\n---\n\n**Status**: \u2705 Complete  \n**Last Updated**: January 17, 2026"