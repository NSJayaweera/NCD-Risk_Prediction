import json
import os

nb_path = r'c:\Users\Isum Enuka\Downloads\osteoporosis-risk-prediction\notebooks\MASTER_Complete_Pipeline.ipynb'

try:
    with open(nb_path, 'r', encoding='utf-8') as f:
        nb = json.load(f)
except Exception as e:
    print(f"Error loading notebook: {e}")
    exit(1)

# Define new content with ALL models and best selection logic
new_train_eval_func = [
    "# ============================================================================\n",
    "# SECTION 5.1: DEFINE MODEL TRAINING FUNCTIONS & HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def get_models_and_params():\n",
    "    # Returns tuple of (models_dict, params_dict)\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Random Forest': RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "        'XGBoost': XGBClassifier(random_state=RANDOM_STATE, verbosity=0, eval_metric='logloss'),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=RANDOM_STATE),\n",
    "        'Bagging': BaggingClassifier(random_state=RANDOM_STATE),\n",
    "        'KNN': KNeighborsClassifier(),\n",
    "        'SVM': SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE),\n",
    "        'Neural Network': 'NN_SPECIAL', # Handled separately\n",
    "        'Stacking': StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "                ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "            ],\n",
    "            final_estimator=LogisticRegression()\n",
    "        ),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=RANDOM_STATE)\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'Logistic Regression': {'C': uniform(0.1, 10), 'solver': ['liblinear', 'lbfgs']},\n",
    "        'Decision Tree': {'max_depth': randint(3, 20), 'min_samples_split': randint(2, 20)},\n",
    "        'Random Forest': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20), 'min_samples_split': randint(2, 10)},\n",
    "        'Gradient Boosting': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10)},\n",
    "        'XGBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 0.3), 'max_depth': randint(3, 10), 'subsample': uniform(0.5, 0.5)},\n",
    "        'AdaBoost': {'n_estimators': randint(50, 300), 'learning_rate': uniform(0.01, 1.0)},\n",
    "        'Bagging': {'n_estimators': randint(10, 100)},\n",
    "        'KNN': {'n_neighbors': randint(3, 20), 'weights': ['uniform', 'distance']},\n",
    "        'SVM': {'C': uniform(0.1, 10), 'gamma': ['scale', 'auto']},\n",
    "        'Stacking': {}, # Usually not tuned in this simple loop\n",
    "        'Extra Trees': {'n_estimators': randint(50, 300), 'max_depth': randint(3, 20)}\n",
    "    }\n",
    "    return models, params\n",
    "\n",
    "def train_evaluate_gender_models(X_tr, y_tr, X_te, y_te, gender_name):\n",
    "    print(f'\\n' + '='*60)\n",
    "    print(f'‚öôÔ∏è TUNING & TRAINING MODELS FOR: {gender_name.upper()}')\n",
    "    print('='*60)\n",
    "\n",
    "    models, params = get_models_and_params()\n",
    "    gender_results = {}\n",
    "    gender_trained_models = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Remove filter: We want to test ALL models now\n",
    "        print(f'   Processing {name}...')\n",
    "        try:\n",
    "            final_model = model\n",
    "            training_history = None\n",
    "\n",
    "            # 1. Hyperparameter Tuning (RandomizedSearchCV)\n",
    "            if name in params and params[name]:\n",
    "                print(f'      -> Tuning hyperparameters...')\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_distributions=params[name],\n",
    "                    n_iter=10,\n",
    "                    cv=3,\n",
    "                    scoring='roc_auc',\n",
    "                    random_state=RANDOM_STATE,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                search.fit(X_tr, y_tr)\n",
    "                final_model = search.best_estimator_\n",
    "                print(f'      -> Best Score: {search.best_score_:.4f}')\n",
    "\n",
    "                # REFIT WITH EVAL_SET FOR LOSS CURVES (Only if model supports it)\n",
    "                if name == 'XGBoost':\n",
    "                    print(f'      -> Refitting with eval_set for Loss Graphs...')\n",
    "                    final_model.fit(\n",
    "                        X_tr, y_tr,\n",
    "                        eval_set=[(X_tr, y_tr), (X_te, y_te)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    training_history = final_model.evals_result()\n",
    "\n",
    "            else:\n",
    "                final_model.fit(X_tr, y_tr)\n",
    "\n",
    "            # 2. Evaluation\n",
    "            y_pred = final_model.predict(X_te)\n",
    "            if hasattr(final_model, 'predict_proba'):\n",
    "                y_pred_proba = final_model.predict_proba(X_te)[:, 1]\n",
    "            else:\n",
    "                y_pred_proba = y_pred # Fallback\n",
    "\n",
    "            # 3. Overfitting Check\n",
    "            y_train_pred = final_model.predict(X_tr)\n",
    "            train_acc = accuracy_score(y_tr, y_train_pred)\n",
    "            test_acc = accuracy_score(y_te, y_pred)\n",
    "            overfit_gap = train_acc - test_acc\n",
    "\n",
    "            # Metrics\n",
    "            roc = roc_auc_score(y_te, y_pred_proba)\n",
    "            f1 = f1_score(y_te, y_pred)\n",
    "\n",
    "            gender_results[name] = {\n",
    "                'accuracy': test_acc,\n",
    "                'roc_auc': roc,\n",
    "                'f1_score': f1,\n",
    "                'train_accuracy': train_acc,\n",
    "                'overfit_gap': overfit_gap,\n",
    "                'model_obj': final_model,\n",
    "                'history': training_history\n",
    "            }\n",
    "            gender_trained_models[name] = final_model\n",
    "            \n",
    "            print(f'      -> Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f} | Gap: {overfit_gap:.4f} | AUC: {roc:.4f}')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'   ‚ö†Ô∏è Error training {name}: {str(e)}')\n",
    "\n",
    "    return gender_results, gender_trained_models\n"
]

new_male_execution_all_models = [
    "# ============================================================================\n",
    "# SECTION 5.3: TUNE, TRAIN & EVALUATE MALE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "male_results, male_models = train_evaluate_gender_models(X_train_m, y_train_m, X_test_m, y_test_m, 'Male')\n",
    "\n",
    "# Leaderboard to find the TRUE Best Male Model\n",
    "male_df = pd.DataFrame(male_results).T.drop('model_obj', axis=1).drop('history', axis=1)\n",
    "male_df = male_df.sort_values('roc_auc', ascending=False)\n",
    "print('\\nüèÜ MALE MODEL LEADERBOARD:')\n",
    "print(male_df)\n",
    "\n",
    "best_male_name = male_df.index[0]\n",
    "best_male_model = male_results[best_male_name]['model_obj']\n",
    "history = male_results[best_male_name]['history']\n",
    "\n",
    "print(f'\\n‚ú® Best Male Model: {best_male_name}')\n",
    "print(f'   ROC-AUC: {male_results[best_male_name][\"roc_auc\"]:.4f}')\n",
    "print(f'   Overfitting Gap: {male_results[best_male_name][\"overfit_gap\"]:.4f}')\n",
    "\n",
    "# 1. PLOT LOSS CURVE (If Available)\n",
    "if history:\n",
    "    training_loss = history['validation_0']['logloss']\n",
    "    validation_loss = history['validation_1']['logloss']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title(f'Male {best_male_name}: Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f'(Loss curve not available for {best_male_name})')\n",
    "\n",
    "# 2. PLOT ROC CURVE (ROG)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "if hasattr(best_male_model, 'predict_proba'):\n",
    "    y_pred_proba = best_male_model.predict_proba(X_test_m)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_m, y_pred_proba)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_val:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Male Model ({best_male_name}): ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
]

new_female_execution_all_models = [
    "# ============================================================================\n",
    "# SECTION 5.4: TUNE, TRAIN & EVALUATE FEMALE MODELS\n",
    "# ============================================================================\n",
    "\n",
    "female_results, female_models = train_evaluate_gender_models(X_train_f, y_train_f, X_test_f, y_test_f, 'Female')\n",
    "\n",
    "# Leaderboard to find the TRUE Best Female Model\n",
    "female_df = pd.DataFrame(female_results).T.drop('model_obj', axis=1).drop('history', axis=1)\n",
    "female_df = female_df.sort_values('roc_auc', ascending=False)\n",
    "print('\\nüèÜ FEMALE MODEL LEADERBOARD:')\n",
    "print(female_df)\n",
    "\n",
    "best_female_name = female_df.index[0]\n",
    "best_female_model = female_results[best_female_name]['model_obj']\n",
    "history = female_results[best_female_name]['history']\n",
    "\n",
    "print(f'\\n‚ú® Best Female Model: {best_female_name}')\n",
    "print(f'   ROC-AUC: {female_results[best_female_name][\"roc_auc\"]:.4f}')\n",
    "print(f'   Overfitting Gap: {female_results[best_female_name][\"overfit_gap\"]:.4f}')\n",
    "\n",
    "# 1. PLOT LOSS CURVE (If Available)\n",
    "if history:\n",
    "    training_loss = history['validation_0']['logloss']\n",
    "    validation_loss = history['validation_1']['logloss']\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Log Loss')\n",
    "    plt.title(f'Female {best_female_name}: Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f'(Loss curve not available for {best_female_name})')\n",
    "\n",
    "# 2. PLOT ROC CURVE (ROG)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "if hasattr(best_female_model, 'predict_proba'):\n",
    "    y_pred_proba = best_female_model.predict_proba(X_test_f)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_f, y_pred_proba)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(fpr, tpr, color='purple', lw=2, label=f'ROC curve (area = {roc_auc_val:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Female Model ({best_female_name}): ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
]

updated_count = 0
for cell in nb['cells']:
    if cell['cell_type'] == 'code':
        source_str = "".join(cell['source'])
        
        if "SECTION 5.1: DEFINE MODEL TRAINING FUNCTIONS" in source_str:
            cell['source'] = new_train_eval_func
            updated_count += 1
            print("Updated Section 5.1")
            
        elif "SECTION 5.3: TUNE, TRAIN & EVALUATE MALE MODELS" in source_str:
            cell['source'] = new_male_execution_all_models
            updated_count += 1
            print("Updated Section 5.3")
            
        elif "SECTION 5.4: TUNE, TRAIN & EVALUATE FEMALE MODELS" in source_str:
            cell['source'] = new_female_execution_all_models
            updated_count += 1
            print("Updated Section 5.4")

with open(nb_path, 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=2)
print(f"Notebook updated successfully. {updated_count} cells modified.")
